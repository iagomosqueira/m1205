% setwd("c:/Projects/M1205/R")
% Sweave("simple_sp_assess.rnw", syntax="SweaveSyntaxNoweb")
% pdflatex simple_sp_assess.tex

% Following Richard Hillary's advice I now use the Hessian of log(r) and log(K)
% And use mvtnorm instead of Cholesky
% In fitsp() we are actually fitting log(r) and log(k)
% It therefore seems sensible to use the hessian of log(r) and log(k)
% when evaluating the uncertainty
% We do this below...

% Jobs
% Richard Hilary suggests estimating log r and log k and using that for the confidence bit
% So get Hessian of log r and log k, get the covariance
% Then use cholesky or mvtnorm to generate estimates of log r and log k
% Then convert to get r and k

% Add plot of fits to compare the three hole scenarios - they look the same
% Fix legend on generic FLsp plot - needs to show points
% INSTEAD OF JUST A HISTOGRAM OF R AND K, WHY NOT PLOT A SURFACE? overlay Inf likelihood area to show
%   sets of parameter values that cannot be.
% What are the consequences for management?

% Careful with the tidied histograms as they have different number of observations
% Make them density or choose same number of obs



% Document describing a very simple example of investigating the impact of reducing the sampling effort using a surplus production model
% We're only going to use 1 fishing history to keep things clear

% Modify for all subsequent chunks: \SweaveOpts{echo=FALSE}
% Or modify for that chunk only, put into <<>>=             @
% fig (FALSE), creates pdf / eps to be inserted from the plot command
% echo (TRUE), R input should be included
% label=xxx, text label for the code chunk, (also the first argument in options if not specified as label), use chunk reference operator <<>> to reference labels
% quiet, all progress messages are suppressed
% debug, input and output of code chunks copied to console
% eval (TRUE), code chunk is evaluated 
% keep.source (FALSE), when echoing, the original source is copied, else deparsed code is copied
% split (FALSE), write text output to seperate file for code chunk
% print (FALSE), wrap code in chunk in a print statement

\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{color}
\usepackage{framed}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\geometry{verbose,a4paper,tmargin=2cm,bmargin=1.5cm,lmargin=2cm,rmargin=3cm}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}
\definecolor{darkblue}{rgb}{0,0,0.5}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\onehalfspacing
\hypersetup{colorlinks, urlcolor=darkblue}

% Title page
\begin{document}
\SweaveOpts{engine=R}
\title{A very simple demonstration of the impact of reducing sampling effort on a stock assessment using a surplus production model}
\author{Finlay Scott <finlay.scott@cefas.co.uk\\
Cefas, Lowestoft, UK}
\date{August 2011}
\maketitle

% Intro. What it does
\section{Introduction}

Blah blah blah

\section{The generic stock}

<<label=libraries,echo=FALSE>>=
library(FLsp)
library(FLH)
library(ggplot2)
library(xtable)
library(mvtnorm)
@

<<label=genericStockParams,echo=FALSE>>=
# Parameters for FLH
Linf <- 120
maxage <- 25
k <- exp(0.5235792+log(Linf)*-0.4540248) # where did these numbers come from?
# selectivity
a1 <- 0.5
sL <- 0.5
sR <- 5
# maturity
mat95 <- 6
# SRR
s <- 0.75
v <- 1e3
# Make the stock
gen1 <- genBRP(age=1:maxage, Linf=Linf, k=k, a1=a1, sL=sL, sR=sR,
                mat95=mat95, s=s, v=v)
@

% Table of parameters
\begin{table}
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{Growth}\\
\hline
$L_{\infty}$ & \Sexpr{Linf}     \\
$k$          & \Sexpr{signif(k,3)}        \\
$maxage$     & \Sexpr{maxage}\\
\hline
\multicolumn{2}{|c|}{Selectivity}\\
\hline
$a1$         & \Sexpr{a1}\\
$sL$         & \Sexpr{sL}\\
$sR$         & \Sexpr{sR}\\
\hline
\multicolumn{2}{|c|}{Maturity}\\
\hline
$mat95$      & \Sexpr{mat95}\\
\hline
\multicolumn{2}{|c|}{SRR}\\
\hline
$s$         & \Sexpr{s}\\
$v$         & \Sexpr{v}\\
\hline
\multicolumn{2}{|c|}{Reference points}\\
\hline
$MSY$       & \Sexpr{signif(c(refpts(gen1)[,"yield"])[4])}\\
$B_{MSY}$       & \Sexpr{signif(c(refpts(gen1)[,"biomass"])[4])}\\
\hline
\end{tabular}
\caption{Parameters for generating the generic stock with $FLH$}
\label{tab:genericStockParams}
\end{table}

\begin{figure}
<<label=genericSelectivityPlot,fig=T,echo=FALSE>>=
print(ggplot(as.data.frame(catch.sel(gen1))) + geom_line(aes(age,data)) + ylab("Catch selectivity") + xlab("Age"))
@
\caption{Catch selectivity curve for the generic stock}
\label{fig:generic_selectivity}
\end{figure}

We're going to make a single stock using Iago's generic life history generator,
$FLH$. The parameters and the resulting reference points are shown in
Table~\ref{tab:genericStockParams}.
The parameters results in a stock with the catch selectivity shown in
Figure~\ref{fig:generic_selectivity}. It's dome shaped but perhaps could be
'domier' for interest.

%*******************************************************************************
% Try a lower maxF
% Might be the reason why a small decrease in r causes the population to crash
% Fishing it too hard

% So if we reduce maxF, will hist of Bcurrents be non zero?
% LogL is less like a singularity!

\section{Fishing scenarios}

When Magnusson and Hilborn (REF) where investigating how information content
of the catch and index histories affected the assessment they used four different
scenarios of fishing mortality:

\begin{itemize}
\item one-way trip, harvest rate gradually increases
\item no change, constant at a somewhat low harvest rate
\item good contrast, stock is fished down to less than half its initial size, then allowed to rebuild
\item rebuild only, stock begins at low abundance and is allowed to rebuild under low F
\end{itemize}

I think that this is a good mix. We should discuss how best to set something like
these up because there are some quirks when using a surplus production model.

For the moment, to keep things simple we're only going to use a single fishing scenario. This is
just to demonstrate the approach.

<<label=single_F_scenario1>>=
maxt <- 40
fmsy <- c(refpts(gen1)[,'harvest'])[4]
maxFfactor <- 2
# if 1, Bootstrapped Bcurrent has no 0s with sine contrast
# if 2, Bootstrapped Bcurrent has some 0s with sine contrast
# if 3, Bootstrapped Bcurrent has many 0s with sine contrast

maxF <- maxFfactor * fmsy

# One way trip
#F1 <- maxF*sin(seq(from=0,to=pi/2,length=maxt))[-1]

# Some contrast
F1 <- maxF*sin(seq(from=0,to=pi,length=maxt))[-1]

# Constant at a high level of F - if maxF = 3*fmsy, almost dead
#F1 <- rep(maxF,maxt-1)

# Add some noise to F to make it interesting
#set.seed(1)
Fnoise_sd <- 0.1
F1 <- F1 * rlnorm(length(F1),0,Fnoise_sd)

@

We'll try the good contrast scenario: starting from an F of 0, F will increase to \Sexpr{maxFfactor} * MSY
before decreasing back to 0.
We'll also multiply the F value by lognormally distributed noise to F with a mean of 1 and
a standard deviation of \Sexpr{Fnoise_sd}.

%The stock will start at virgin and we'll increase the harvest rate in a non-linear fashion,
%starting in year 2.


You can see this in Figure~\ref{fig:Fscenario}.

\begin{figure}
<<label=F_plot1,echo=FALSE,fig=TRUE>>=
print(ggplot(data.frame(time=2:maxt, F=F1)) + geom_line(aes(x=time,y=F)) +
	geom_line(aes(x=time,y=FMSY),data=data.frame(time=2:maxt, FMSY=fmsy)))
@
\caption{Fishing mortality scenario. F increases from 0 to \Sexpr{maxFfactor} x $F_{MSY}$. Horizontal line is $F_{MSY}$.}
\label{fig:Fscenario}
\end{figure}

\section{Projecting the stock forward}

Using the glory of $FLash$ we can project the stock forward under this fishing
scenario.

<<label=srr_sd, echo=FALSE>>=
srr_sd <- 0.3
@

We'll put a small amount of lognormally distributed noise onto the recruitment with
a mean of 1 and a standard deviation of \Sexpr{srr_sd}.

%The projection will be entirely deterministic, i.e. no noise on the
%SRR. Again, this is just to keep things simple for now.

Here we convert our generic stock (currently as a $FLBRP$ object) into an $FLStock$,
set a control object and then project forward:

<<label=fwd1>>=
stk1 <- as(gen1, 'FLStock')
stk1 <- window(stk1,end=maxt)
ctrl_F1 <- fwdControl(data.frame(year=2:maxt, quantity="f",val=F1))
srr_residuals <- FLQuant(rlnorm(dims(stk1)$year * dims(stk1)$iter,0,srr_sd),
									dimnames=list(age=1,year=dimnames(stock.n(stk1))$year,iter=dimnames(stock.n(stk1))$iter))
stk1 <- fwd(stk1, ctrl=ctrl_F1, sr=list(model=model(gen1), params=params(gen1)), sr.residuals=srr_residuals)
#stk1 <- fwd(stk1, ctrl=ctrl_F1, sr=list(model=model(gen1), params=params(gen1)))
@

The resulting stock object can be seen in Figure~\ref{fig:simple_proj_results}.

\begin{figure}
<<label=stk_plot,fig=T,echo=FALSE>>=
print(plot(stk1))
@
\caption{The results of the projection}
\label{fig:simple_proj_results}
\end{figure}


\section{Getting the catch and index data for the assessment}

For the surplus production model we need to generate total catch and some
kind of index data.
We are going to be kind to the surplus production model and assume that knowledge
is perfect, i.e. no observation error.

Catch is easy to set up:

<<label=catch>>=
catch <- catch(stk1)
@

The index is slightly trickier. We're going to assume the index comes from a
survey vessel so we need to set up a catch selectivity for the survey.
It's going to be sigmoid that is fully selected at age 4 (Figure~\ref{fig:survey_sel})..

<<label=survey_sel>>=
survey_sel <- rep(NA,maxage)
sfull <- 5
sright <- 15
sleft <- 1
sleftright <- rep(NA,maxage)
sleftright[(1:maxage)<=sfull] <- sleft
sleftright[(1:maxage) > sfull] <- sright
ssel <- exp( -((1:maxage)-sfull)^2 / (exp(sleftright)) )
dnames=dimnames(stock.n(stk1))
dnames$iter <- "1"
sselq <- FLQuant(ssel,dimnames=dnames)
@

\begin{figure}
<<label=survey_sel_plot, echo=FALSE,fig=TRUE>>=
print(ggplot(as.data.frame(sselq)) + geom_line(aes(age,data)) + ylab("Survey catch selectivity") + xlab("Age"))
@
\caption{The selectivity curve of the survey vessel}
\label{fig:survey_sel}
\end{figure}

We now apply this selectivity to the population, and sum to get the index of abundance.
We assume that the survey takes place half way through the year
and we'll scale the abundance down by a 1000:

<<label=survey_generation>>=
index <- apply(sweep(stk1@stock.n * exp(stk1@m/2) * stk1@stock.wt,1:5,sselq,"*"),2:6,sum)/1000
@

\section{The assessment}

%We'll first of all run the 'best possible' assessment. There is no stochasticity
%anywhere and the model has almost perfect knowledge of the catch and the index
%of abundance (I say almost because the selectivity of the survey comes into play).

We're going to use the full data set for both catch and index, something
that we play around with later on.

First we create the $FLsp$ object and then fit it.
To fit it we need set some limits on the parameters, the narrower these limits the
better the chance of fitting.
For example, the minimum value of $k$ is unlikely to be smaller than the maximum catch.
We can also set the number of iterations for the solver. Scenarios with less information
will need more iterations.
%Because it's a one way trip we really
%need to crank up the iterations on the solver to make sure it gets there. It might take a while...

<<label=set_seed_FLsp,echo=FALSE>>=
set.seed(1234)
@

<<label=best_assessment, quiet=TRUE>>=
flsp <- FLsp(catch=catch, index=index)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp <- fitsp(flsp, lower=lower, upper=upper,
 control=DEoptim.control(itermax=2000, trace=200))
@

Even with contrast in the data, the likelihood profile can be pretty unpleasant to fit over,
depending on fishing pressure.
We can see this by holding $k$ at the estimated value and looking at the likelihood
as $r$ varies. The range of of has be kept very small to see that the spike is really a
'hill' and not a singularity (Figure~\ref{fig:r_profile}).

\begin{figure}
<<label=plot_r_profile,echo=FALSE,fig=TRUE>>=
profile(flsp,fixed=list(k=params(flsp)['k',]),maxsteps=1000,range=0.01)
@
\label{fig:r_profile}
\caption{Likelihood profile as $r$ changes with $k$ fixed at best estimated value of \Sexpr{signif(c(flsp@params['k',]))}.}
\end{figure}

We can also check that the fit is good by looking at the Hessian matrix. $FLsp$ uses
automatic differentiation (implemented using ADOLC) to get exact Hessian matrix.
Internally, $FLsp$ actually estimates $log(r)$ and $log(k)$ so the Hessian relates
the changes in the gradient to changes in $log(r)$ and $log(k)$, not $r$ and $k$.
The values of the Hessian, along with the other parameters can be seen in Table~\ref{tab:all_results}.

(Note: originally I used the Hessian relative to $r$ and $k$, not the logged values.
I was able to run the following confidence analysis but it was producing a lot of
'impossible' $r$ and $k$ combinations, i.e. ones that had an negative infinity likelihood.
Following Richard Hilary's advice I have now swapped to using the Hessian of the logged
parameters, which makes sense seeing as I am estimating the logged parameters).

\section{Missing data and the impact on confidence}

The first question to answer is: what happens if we only have index data every
other year?
We would expect the confidence in our assessment to go down.
Does it? Let's find out.

<<label=setting_up_index_holes>>=
idxyrs <- as.numeric(dimnames(index)$year)
nayears1 <- seq(from=idxyrs[1],to=idxyrs[length(idxyrs)],by=2)
index_holes1 <- index
index_holes1[,nayears1] <- NA
@

Now set up another $FLsp$ object and fit.

<<label=holes1_assessment, quiet=TRUE>>=
flsp_holes1 <- FLsp(catch=catch, index=index_holes1)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp_holes1 <- fitsp(flsp_holes1, lower=lower, upper=upper,
 control=DEoptim.control(itermax=2000, trace=200))
@

Although the estimated parameter values have not changed by much, the magnitude values in the
variance-covariance matrix of the estimated parameters has increased
which means that we are less confident in the results of the assessment (Table~\ref{tab:all_results}).

Let's do it again, this time with even more holes. This time there is one year of data to
two years of missing data:

<<label=setting_up_index_holes2>>=
nayears2 <- seq(from=idxyrs[1],to=idxyrs[length(idxyrs)],by=3)
index_holes2 <- index
index_holes2[,!(idxyrs %in% nayears2)] <- NA
flsp_holes2 <- FLsp(catch=catch, index=index_holes2)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp_holes2 <- fitsp(flsp_holes2, lower=lower, upper=upper,
 control=DEoptim.control(itermax=2000, trace=200))
@

The results of all three models can be see in Table~\ref{tab:all_results}.

The estimated parameter values and the corresponding values of $B_{current}$, $MSY$ and $B_{MSY}$ are all very similar.
However, again the magnitude of the values in the variance-covariance matrix have increased (Table~\ref{tab:all_results}), again
suggesting tha confidence in these results has decreased.

What are the consequences of this decreased confidence?

% Ubertable of results
<<label=df_of_three_spresults,echo=FALSE>>=
temp <- data.frame(Measure=c("r","k","Hess_rr","Hess_rk","Hess_kr","Hess_kk","Vcov_rr","Vcov_rk","Vcov_kr","Vcov_kk","Bcurrent","MSY","Bmsy"),
									All_index_yrs=c(c(flsp@params),c(flsp@hessian_log),c(flsp@vcov_log),speedy_bcurrent(flsp),Msy(flsp),Bmsy(flsp)),
									Holes_index_1=c(c(flsp_holes1@params),c(flsp_holes1@hessian_log),c(flsp_holes1@vcov_log),speedy_bcurrent(flsp_holes1),Msy(flsp_holes1),Bmsy(flsp_holes1)),
									Holes_index_2=c(c(flsp_holes2@params),c(flsp_holes2@hessian_log),c(flsp_holes2@vcov_log),speedy_bcurrent(flsp_holes2),Msy(flsp_holes2),Bmsy(flsp_holes2))
									)
@

<<label=write_table, results=tex, echo=FALSE>>=
print(xtable(temp,
				caption="Assessment results for three data sets with increasing holes in index data",
				label="tab:all_results",
				digits=-2
				))
#				floating.environment = 'sidewaystable')
@

\section{Consequences of Confidence}

We can use the variance-covariance matrices to produce bootstrapped estimates of $r$ and $k$,
and therefore bootstrapped estimates of $B_{current}$, $MSY$ and $B_{MSY}$.

%This can be done using a Cholesky decomposition of the variance-covariance matrix.
%This method makes a bunch of assumptions blah blah.

Originally, I used the Colesky decompostion of the variance-covariance matrix
and multiplied it by i.i.ds to get the bootstrapped estiamtes of $r$ and $k$.
However, I now use the \emph{rmvnorm()} function in the \emph{mvtnorm} library
to get the bootstrapped estimates.

%Do I need to? Can I just use dmvnorm to get the distribution

The method can be put into a function:

# Check this function!

<<label=conf_func_mvtnorm>>=
conf_sp <- function(sp,nsam=1000)
{
	# Do we have vcov
	vcovm <- matrix(sp@vcov_log,nrow=dim(sp@vcov_log)[1])
	#d <- dmvnorm(x=c(0,0.5), sigma=vcovm)
  #p <- pmvnorm(lower=c(0,0), upper=c(1,1),sigma=vcovm)
  #rmv <- rmvnorm(n = nsam, sigma=vcovm)
  #apply(rmv,2,mean) # why isn't mean 0?
  rmv <- rmvnorm(n = nsam, mean=c(log(params(sp)['r',]),log(params(sp)['k',])),sigma=vcovm)
  #apply(rmv,2,mean) # why isn't mean 0?
	# Make a new object
	sp_sim <- sp
	params(sp_sim) <- propagate(params(sp_sim), nrow(rmv))
	params(sp_sim)[] <- t(exp(rmv))
	return(sp_sim)
}
@

This function can then be used to generate bootstrapped $FLsp$ objects.

<<label=bootstrap_assessments>>=
nsam <- 1000
flsp_boot <- conf_sp(flsp,nsam)
flsp_holes1_boot <- conf_sp(flsp_holes1,nsam)
flsp_holes2_boot <- conf_sp(flsp_holes2,nsam)
@

\begin{figure}
<<label=histo_r_k,echo=FALSE,fig=T>>=
rkdf <- rbind(cbind(index="full",as.data.frame(sweep(flsp_boot@params,1,flsp@params,"/"))),
							cbind(index="holes1",as.data.frame(sweep(flsp_holes1_boot@params,1,flsp_holes1@params,"/"))),
							cbind(index="holes2",as.data.frame(sweep(flsp_holes2_boot@params,1,flsp_holes2@params,"/"))))

print(ggplot(rkdf,aes(x=data)) + geom_histogram() + facet_grid(index ~ params))

#ggplot(rkdf,aes(x=data)) + geom_histogram() + facet_grid(index ~ params)
ggplot(rkdf[rkdf$index=="full",],aes(x=data)) + geom_histogram()

hist(rkdf[rkdf$index=="full" & rkdf$params=="r","data"])
hist(rkdf[rkdf$index=="full" & rkdf$params=="k","data"])
# Estimates of k have gone very wrong....

@
\caption{Histograms of the bootstrapped $r$ and $k$ values, relative to the 'true' value}
\label{fig:rk_hist}
\end{figure}

Histograms of the bootstrapped values of $r$ and $k$, relative to the 'true' value can be seen in Figure~\ref{fig:rk_hist}.

\begin{figure}
<<label=boot_MSYetc,echo=FALSE, fig=TRUE>>=
msyetcdf <- rbind(data.frame(index="full", MSY=Msy(flsp_boot),BMSY = Bmsy(flsp_boot), BC = speedy_bcurrent(flsp_boot)),
						data.frame(index="holes1", MSY=Msy(flsp_holes1_boot),BMSY = Bmsy(flsp_holes1_boot), BC = speedy_bcurrent(flsp_holes1_boot)),
						data.frame(index="holes2", MSY=Msy(flsp_holes2_boot),BMSY = Bmsy(flsp_holes2_boot), BC = speedy_bcurrent(flsp_holes2_boot)))
# melt this to single value column
meltdf <- melt(msyetcdf,id.vars="index")
print(ggplot(meltdf,aes(x=value)) + geom_histogram() + facet_grid(index ~ variable, scales="free"))
@
\caption{Histograms of derived measures for the three index holes scenarios}
\label{fig:hist_msy_holes}
\end{figure}

We can do the same thing for the derived measures $B_{current}$, $MSY$ and $B_{MSY}$ (Figure~\ref{fig:hist_msy_holes}).

This clearly shows that as the number of holes in the index data increases, the confidence in the estimated parameters $r$ and $k$ and
the derived measures decreases.

Some of the bootstrapped pairs of values of $r$ and $k$ actually result in a value
of 0 for $B_{current}$, i.e. the combination of $r$ and $k$ has led to the population
crashing.

What does this mean?
Although the histograms show the uncertainty
in the best estimate of $B_{current}$, we know that the population
hasn't actually crashed.

This is because our bootstrapping method is limited. The generated pairs of values of $r$ and $k$ might be
statistically consistent with the covariance matrix, but the covariance matrix is
only an approximation to the 'true' confidence in the parameters. Clearly we cannot have
pairs of parameter values that we know are not possible.
For example, there is no point in saying that there
is a 10\% chance that the population is extinct when we know that in reality is is still alive.

We therefore have to do some tidying up of the generated pairs of values.

<<label=getLL_of_rk_pairs_holes, echo=FALSE>>=
ll_holes2 <- speedy_ll(flsp_holes2_boot)
ll_holes1 <- speedy_ll(flsp_holes1_boot)
ll_full <- speedy_ll(flsp_boot)
@

First let's take a look at the likelihood of each pair of parameter values for the
scenario where there are most hole in the index. Out of the
\Sexpr{length(c(ll_holes2))} values, \Sexpr{sum(is.nan(c(ll_holes2)))} are NaN.
This happens when the population crashes.
This means that there are \Sexpr{sum(is.nan(c(ll_holes2)))} pairs of $r$ and $k$ values that
lead to an impossible outcome.

<<label=min_n,echo=FALSE>>=
minn <- 1000
# What is the minimum number of pairs
minpair <- max(minn,min(sum(!is.nan(ll_holes2)),sum(!is.nan(ll_holes1)),sum(!is.nan(ll_full))))
@

This still leaves \Sexpr{length(c(ll_holes2)) - sum(is.nan(c(ll_holes2)))} pairs that
are 'good'. Obviously we want as many as possible.
At a rough guess I would say we want at least 200 'good' pairs to give us
a rough picture of the confidence in our parameter estimates.
For comparison, we want the number to be the same for all three scenarios.

One method of building a picture of the confidence is by randomly sampling from the 'good' pairs
based on their likelihood (the more likely pairs have a higher chance of being picked).
We do this by scaling the sum of the likelihoods between 0 and 1,
 picking \Sexpr{minn} values from a uniform distribution between 0 and 1 and
using this value to select which pair. The pairs of $r$ and $k$ that lead to extinction
have a probability of 0, i.e. are never picked.

<<label=pick_pairs>>=
pick_pairs <- function(flsp,minn)
{
	lls <- speedy_ll(flsp)
	lls[is.nan(lls)] <- NA
	lls_shift <- lls - min(lls,na.rm=T)
	scaled_lls <- (lls_shift) / sum(lls_shift,na.rm=T)
	# Set prob of NA to 0 so they never get picked
	scaled_lls[is.na(scaled_lls)] <- 0
	# Pick the pairs
	rkpairs <- sapply(runif(minn),function(x) which(cumsum(c(scaled_lls))>=x)[1])
	return(rkpairs)
}

flsp_full_boot2 <- iter(flsp_boot,pick_pairs(flsp_boot,minn=minpair))
flsp_holes1_boot2 <- iter(flsp_holes1_boot,pick_pairs(flsp_holes1_boot,minn=minpair))
flsp_holes2_boot2 <- iter(flsp_holes2_boot,pick_pairs(flsp_holes2_boot,minn=minpair))
@

Actually, there is no real need to do the above. It is only necessary if a real likelihood
is returned when the population crashes, when really the likelihood should be $NaN$ or $Inf$.
This is often done if the solver demands the function it is solving over is continuous.
In these cases, the likelihood should be very heavily penalised and the probability of being
selected in the above routine set to 0.

As $FLsp$ currently stands, when the population crashes the likelihood is $NaN$ or $Inf$.
We could therefore just sample from the non-$NaN$ likelihood pairs, or even just pick the first
\Sexpr{minn} non-$NaN$ pairs. The above method is included for completeness.

% Why can't we just randomly pick from the good pairs? Why do we need to pick them
% based on their likelihood? There are already many likely pairs than unlikely ones
% so just randomly sampling from them will preserve the structure, no?

However, when I've tried it I've found that the resulting confidences look oddly truncated.
We try the 'just drop the impossible pairs method' below.

<<label=dropNaNs>>=
# Or we just miss out the NaNs
drop_NaNs <- function(flsp)
{
	lls <- speedy_ll(flsp)
	dropthese <- which(is.nan(lls))
	flsp@params <- flsp@params[,-dropthese]
	return(flsp)
}

flsp_full_boot2 <- drop_NaNs(flsp_boot)
flsp_holes1_boot2 <- drop_NaNs(flsp_holes1_boot)
flsp_holes2_boot2 <- drop_NaNs(flsp_holes2_boot)
@

Actually, there is no real need to do the above. It is only necessary if a real likelihood
is returned when the population crashes, when really the likelihood should be $NaN$ or $Inf$.
This is often done if the solver demands the function it is solving over is continuous.
In these cases, the likelihood should be very heavily penalised and the probability of being
selected in the above routine set to 0.

As $FLsp$ currently stands, when the population crashes the likelihood is $NaN$ or $Inf$.
We could therefore just sample from the non-$NaN$ likelihood pairs, or even just pick the first
\Sexpr{minn} non-$NaN$ pairs. The above method is included for completeness.

% Laurie said to plot the sorted likelihoods.
% He said it would be sigmoid shape - think this is only the case when you have a continuous LL
% i.e. set minimim B to 1e-9 or something - so you can get LLs even when r and k are weird
% So he weights the likelihood. i..e. if pop crash, massively negative LL
% We don't do that because we are using a min pop of 0, so we get NaN if pop crashes
% Also the sigmoid is only approximate - if all the rk pairs are good then no front tail.


% So the next step only matters if we have Bmin != 0?
% and why do it at all?
% Scale between 0 and 1.
% Pick uniform random number between 0 and 1. Sample from LLs, with replacement.
% How will this be different to our original spread of rkpairs?
% Surely we still have a chance of picking a 'dud' rkpair?
% This will only work if all the 'dud' rkpair have the same likelihood (probably heavily weighted)
% and the prob of picking it is 0.

% So this whole rescale thing is uncessary of we return -Inf, cut out the NaNs and keep
% generating covars until we have enough

We can now plot again the various measures (Figure~\ref{fig:hist_msy_holes_tidy}).

\begin{figure}
<<label=boot_MSYetc_tidy,echo=FALSE, fig=TRUE>>=
msyetcdf2 <- rbind(data.frame(index="full", MSY=Msy(flsp_full_boot2),BMSY = Bmsy(flsp_full_boot2), BC = speedy_bcurrent(flsp_full_boot2)),
						data.frame(index="holes1", MSY=Msy(flsp_holes1_boot2),BMSY = Bmsy(flsp_holes1_boot2), BC = speedy_bcurrent(flsp_holes1_boot2)),
						data.frame(index="holes2", MSY=Msy(flsp_holes2_boot2),BMSY = Bmsy(flsp_holes2_boot2), BC = speedy_bcurrent(flsp_holes2_boot2)))
# melt this to single value column
meltdf2 <- melt(msyetcdf2,id.vars="index")
print(ggplot(meltdf2,aes(x=value)) + geom_histogram() + facet_grid(index ~ variable, scales="free"))
@
\caption{Histograms of derived measures for the three index holes scenarios with the $r$-$k$ pairs tidied up.}
\label{fig:hist_msy_holes_tidy}
\end{figure}

The quantiles of the various measures can be seen in Table~\ref{tab:quant}.

Quick tests have shown that almost all bootstrapped Bcurrent values are 0 for
with the 'one-way-trip' scenario when maxFfactor = 2.

<<label=quantiles, echo=FALSE>>=
temp <- melt(msyetcdf2, id.vars="index")
qfunc <- function(sub)
	quantile(sub$value, c(0.05,0.25,0.5,0.75,0.95))

qu <- ddply(temp, .(index,variable), qfunc)
# Need to reshape this
qum <- melt(qu, id.vars=c("index","variable"), variable_name="quantile")
qudf <- cast(qum, index + variable ~ quantile)
qudf <- qudf[order(qudf$variable),]
@

<<label=write_qudftable, results=tex, echo=FALSE>>=
print(xtable(qudf,
				caption="Quantiles of the three measures for the three holes scenario.",
				label="tab:quant",
				digits=2
				))
#				floating.environment = 'sidewaystable')
@

\section{Alternative confidence}
<<>>=

sp <- flsp

slotNames(sp)
sp@vcov_log
sp@vcov
# With log vcov K is smaller than r. Is this right?
library(mvtnorm)
vcov_log <- matrix(sp@vcov_log,nrow=2)
vcov <- matrix(sp@vcov,nrow=2)
sam <- rmvnorm(2000, sigma=vcov)
sam_log <- rmvnorm(2000, sigma=vcov_log)
# Correct for mean
sam <- sweep(sam,2,params(sp),"+")
sam_log <- sweep(sam_log,2,log(params(sp)),"+")

#par(mfrow=c(3,2))
#hist(sam[,1],main="r")
#hist(sam[,2],main="k")
#hist(sam_log[,1],main="log r")
#hist(sam_log[,2],main="log k")
#hist(exp(sam_log[,1]),main="exp log r")
#hist(exp(sam_log[,2]),main="exp log k")

flsp_holes2_boot2 <- flsp_holes2_boot
flsp_holes2_boot2@params['r',] <- exp(sam_log[,1])
flsp_holes2_boot2@params['k',] <- exp(sam_log[,2])

flsp_holes2_boot3 <- flsp_holes2_boot
flsp_holes2_boot3@params['r',] <- sam[,1]
flsp_holes2_boot3@params['k',] <- sam[,2]


ll_holes22 <- speedy_ll(flsp_holes2_boot2)
bc22 <- speedy_bcurrent(flsp_holes2_boot2)
ll_holes23 <- speedy_ll(flsp_holes2_boot3)
bc23 <- speedy_bcurrent(flsp_holes2_boot3)


par(mfrow=c(2,2))
hist(ll_holes22,main="ll2")
hist(bc22,main="bc2")
hist(ll_holes23,main="ll3")
hist(bc23,main="bc3")



@

\section{References}

Sweave
Polacheck


\end{document}

