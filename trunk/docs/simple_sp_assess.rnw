% setwd("c:/Projects/M1205")
% Sweave("simple_sp_assess.rnw", syntax="SweaveSyntaxNoweb")
% pdflatex simple_sp_assess.tex

% Jobs
% Add plot of fits to compare the three hole scenarios - they look the same
% Fix legend on generic FLsp plot - needs to show points
% INSTEAD OF JUST A HISTOGRAM OF R AND K, WHY NOT PLOT A SURFACE? overlay Inf likelihood area to show
#   sets of parameter values that cannot be.
% What happens when Bcurrent = 0
% What are the consequences for management?

% Document describing a very simple example of investigating the impact of reducing the sampling effort using a surplus production model
% We're only going to use 1 fishing history to keep things clear

% Modify for all subsequent chunks: \SweaveOpts{echo=FALSE}
% Or modify for that chunk only, put into <<>>=
% fig (FALSE), creates pdf / eps to be inserted from the plot command
% echo (TRUE), R input should be included
% label=xxx, text label for the code chunk, (also the first argument in options if not specified as label), use chunk reference operator <<>> to reference labels
% quiet, all progress messages are suppressed
% debug, input and output of code chunks copied to console
% eval (TRUE), code chunk is evaluated 
% keep.source (FALSE), when echoing, the original source is copied, else deparsed code is copied
% split (FALSE), write text output to seperate file for code chunk
% print (FALSE), wrap code in chunk in a print statement

\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{color}
\usepackage{framed}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\geometry{verbose,a4paper,tmargin=2cm,bmargin=1.5cm,lmargin=2cm,rmargin=3cm}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}
\definecolor{darkblue}{rgb}{0,0,0.5}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\onehalfspacing
\hypersetup{colorlinks, urlcolor=darkblue}

% Title page
\begin{document}
\SweaveOpts{engine=R}
\title{A very simple demonstration of the impact of reducing sampling effort on a stock assessment using a surplus production model}
\author{Finlay Scott <finlay.scott@cefas.co.uk\\
Cefas, Lowestoft, UK}
\date{August 2011}
\maketitle

% Intro. What it does
\section{Introduction}

Blah blah blah

\section{The generic stock}

<<label=libraries,echo=FALSE>>=
library(FLsp)
library(FLH)
library(ggplot2)
library(xtable)
@

<<label=genericStockParams,echo=FALSE>>=
# Parameters for FLH
Linf <- 120
maxage <- 25
k <- exp(0.5235792+log(Linf)*-0.4540248) # where did these numbers come from?
# selectivity
a1 <- 0.5
sL <- 0.5
sR <- 5
# maturity
mat95 <- 6
# SRR
s <- 0.75
v <- 1e3
# Make the stock
gen1 <- genBRP(age=1:maxage, Linf=Linf, k=k, a1=a1, sL=sL, sR=sR,
                mat95=mat95, s=s, v=v)
@

% Table of parameters
\begin{table}
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{Growth}\\
\hline
$L_{\infty}$ & \Sexpr{Linf}     \\
$k$          & \Sexpr{signif(k,3)}        \\
$maxage$     & \Sexpr{maxage}\\
\hline
\multicolumn{2}{|c|}{Selectivity}\\
\hline
$a1$         & \Sexpr{a1}\\
$sL$         & \Sexpr{sL}\\
$sR$         & \Sexpr{sR}\\
\hline
\multicolumn{2}{|c|}{Maturity}\\
\hline
$mat95$      & \Sexpr{mat95}\\
\hline
\multicolumn{2}{|c|}{SRR}\\
\hline
$s$         & \Sexpr{s}\\
$v$         & \Sexpr{v}\\
\hline
\multicolumn{2}{|c|}{Reference points}\\
\hline
$MSY$       & \Sexpr{signif(c(refpts(gen1)[,"yield"])[4])}\\
$B_{MSY}$       & \Sexpr{signif(c(refpts(gen1)[,"biomass"])[4])}\\
\hline
\end{tabular}
\caption{Parameters for generating the generic stock with $FLH$}
\label{tab:genericStockParams}
\end{table}

\begin{figure}
<<label=genericSelectivityPlot,fig=T,echo=FALSE>>=
print(ggplot(as.data.frame(catch.sel(gen1))) + geom_line(aes(age,data)) + ylab("Catch selectivity") + xlab("Age"))
@
\caption{Catch selectivity curve for the generic stock}
\label{fig:generic_selectivity}
\end{figure}

We're going to make a single stock using Iago's generic life history generator,
$FLH$. The parameters and the resulting reference points are shown in
Table~\ref{tab:genericStockParams}.
The parameters results in a stock with the catch selectivity shown in
Figure~\ref{fig:generic_selectivity}. It's dome shaped but perhaps could be
'domier' for interest.

%*******************************************************************************
% Try a lower maxF
% Might be the reason why a small decrease in r causes the population to crash
% Fishing it too hard

% So if we reduce maxF, will hist of Bcurrents be non zero?
% LogL is less like a singularity!

\section{Fishing scenarios}

When Magnusson and Hilborn (REF) where investigating how information content
of the catch and index histories affected the assessment they used four different
scenarios of fishing mortality:

\begin{itemize}
\item one-way trip, harvest rate gradually increases
\item no change, constant at a somewhat low harvest rate
\item good contrast, stock is fished down to less than half its initial size, then allowed to rebuild
\item rebuild only, stock begins at low abundance and is allowed to rebuild under low F
\end{itemize}

I think that this is a good mix. We should discuss how best to set something like
these up because there are some quirks when using a surplus production model.

For the moment, to keep things simple we're only going to use a single fishing scenario. This is
just to demonstrate the approach.

<<label=single_F_scenario1>>=
maxt <- 40
fmsy <- c(refpts(gen1)[,'harvest'])[4]
maxFfactor <- 2
# if 1, Bootstrapped Bcurrent has no 0s with sine contrast
# if 2, Bootstrapped Bcurrent has some 0s with sine contrast
# if 3, Bootstrapped Bcurrent has many 0s with sine contrast

maxF <- maxFfactor * fmsy

# One way trip
#F1 <- maxF*sin(seq(from=0,to=pi/2,length=maxt))[-1]

# Some contrast
F1 <- maxF*sin(seq(from=0,to=pi,length=maxt))[-1]

# Constant at a high level of F - if maxF = 3*fmsy, almost dead
#F1 <- rep(maxF,maxt-1)

# Add some noise to F to make it interesting
set.seed(0)
Fnoise_sd <- 0.1
F1 <- F1 * rlnorm(length(F1),0,Fnoise_sd)

@

We'll try the good contrast scenario: starting from an F of 0, F will increase to \Sexpr{maxFfactor} * MSY
before decreasing back to 0.
We'll also multiply the F value by lognormally distributed noise to F with a mean of 1 and
a standard deviation of \Sexpr{Fnoise_sd}.

%The stock will start at virgin and we'll increase the harvest rate in a non-linear fashion,
%starting in year 2.


You can see this in Figure~\ref{fig:Fscenario}.

\begin{figure}
<<label=F_plot1,echo=FALSE,fig=TRUE>>=
print(ggplot(data.frame(time=2:maxt, F=F1)) + geom_line(aes(x=time,y=F)) +
	geom_line(aes(x=time,y=FMSY),data=data.frame(time=2:maxt, FMSY=fmsy)))
@
\caption{Fishing mortality scenario. F increases from 0 to \Sexpr{maxFfactor} x $F_{MSY}$. Horizontal line is $F_{MSY}$.}
\label{fig:Fscenario}
\end{figure}

\section{Projecting the stock forward}

Using the glory of $FLash$ we can project the stock forward under this fishing
scenario.

<<label=srr_sd, echo=FALSE>>=
srr_sd <- 0.1
@

We'll put a small amount of lognormally distributed noise onto the recruitment with
a mean of 1 and a standard deviation of \Sexpr{srr_sd}.

%The projection will be entirely deterministic, i.e. no noise on the
%SRR. Again, this is just to keep things simple for now.

Here we convert our generic stock (currently as a $FLBRP$ object) into an $FLStock$,
set a control object and then project forward:

<<label=fwd1>>=
stk1 <- as(gen1, 'FLStock')
stk1 <- window(stk1,end=maxt)
ctrl_F1 <- fwdControl(data.frame(year=2:maxt, quantity="f",val=F1))
srr_residuals <- FLQuant(rlnorm(dims(stk1)$year * dims(stk1)$iter,0,srr_sd),
									dimnames=list(age=1,year=dimnames(stock.n(stk1))$year,iter=dimnames(stock.n(stk1))$iter))
stk1 <- fwd(stk1, ctrl=ctrl_F1, sr=list(model=model(gen1), params=params(gen1)), sr.residuals=srr_residuals)
#stk1 <- fwd(stk1, ctrl=ctrl_F1, sr=list(model=model(gen1), params=params(gen1)))
@

The resulting stock object can be seen in Figure~\ref{fig:simple_proj_results}.

\begin{figure}
<<label=stk_plot,fig=T,echo=FALSE>>=
print(plot(stk1))
@
\caption{The results of the projection}
\label{fig:simple_proj_results}
\end{figure}


\section{Getting the catch and index data for the assessment}

For the surplus production model we need to generate total catch and some
kind of index data.
We are going to be kind to the surplus production model and assume that knowledge
is perfect, i.e. no observation error.

Catch is easy to set up:

<<label=catch>>=
catch <- catch(stk1)
@

The index is slightly trickier. We're going to assume the index comes from a
survey vessel so we need to set up a catch selectivity for the survey.
It's going to be sigmoid that is fully selected at age 4 (Figure~\ref{fig:survey_sel})..

<<label=survey_sel>>=
survey_sel <- rep(NA,maxage)
sfull <- 5
sright <- 15
sleft <- 1
sleftright <- rep(NA,maxage)
sleftright[(1:maxage)<=sfull] <- sleft
sleftright[(1:maxage) > sfull] <- sright
ssel <- exp( -((1:maxage)-sfull)^2 / (exp(sleftright)) )
dnames=dimnames(stock.n(stk1))
dnames$iter <- "1"
sselq <- FLQuant(ssel,dimnames=dnames)
@

\begin{figure}
<<label=survey_sel_plot, echo=FALSE,fig=TRUE>>=
print(ggplot(as.data.frame(sselq)) + geom_line(aes(age,data)) + ylab("Survey catch selectivity") + xlab("Age"))
@
\caption{The selectivity curve of the survey vessel}
\label{fig:survey_sel}
\end{figure}

We now apply this selectivity to the population, and sum to get the index of abundance.
We assume that the survey takes place half way through the year
and we'll scale the abundance down by a 1000:

<<label=survey_generation>>=
index <- apply(sweep(stk1@stock.n * exp(stk1@m/2) * stk1@stock.wt,1:5,sselq,"*"),2:6,sum)/1000
@

\section{The assessment}

%We'll first of all run the 'best possible' assessment. There is no stochasticity
%anywhere and the model has almost perfect knowledge of the catch and the index
%of abundance (I say almost because the selectivity of the survey comes into play).

We're going to use the full data set for both catch and index, something
that we play around with later on.

First we create the $FLsp$ object and then fit it.
To fit it we need set some limits on the parameters, the narrower these limits the
better the chance of fitting.
For example, the minimum value of $k$ is unlikely to be smaller than the maximum catch.
We can also set the number of iterations for the solver. Scenarios with less information
will need more iterations.
%Because it's a one way trip we really
%need to crank up the iterations on the solver to make sure it gets there. It might take a while...

<<label=set_seed_FLsp,echo=FALSE>>=
set.seed(1234)
@

<<label=best_assessment, quiet=TRUE>>=
flsp <- FLsp(catch=catch, index=index)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp <- fitsp(flsp, lower=lower, upper=upper,
 control=DEoptim.control(itermax=1000, trace=200))
@

Even with contrast in the data, the likelihood profile can be pretty unpleasant to fit over,
depending on fishing pressure.
We can see this by holding $k$ at the estimated value and looking at the likelihood
as $r$ varies. The range of of has be kept very small to see that the spike is really a
'hill' and not a singularity (Figure~\ref{fig:r_profile}).

\begin{figure}
<<label=plot_r_profile,echo=FALSE,fig=TRUE>>=
profile(flsp,fixed=list(k=params(flsp)['k',]),maxsteps=1000,range=0.01)
@
\label{fig:r_profile}
\caption{Likelihood profile as $r$ changes with $k$ fixed at best estimated value of \Sexpr{signif(c(flsp@params['k',]))}.}
\end{figure}

We can also check that the fit is good by looking at the Hessian matrix. $FLsp$ uses
automatic differentiation (implemented using ADOLC) to get exact Hessian matrix.
The values of the Hessian, along with the other parameters can be seen in Table~\ref{tab:all_results}.

%\begin{table}
%\begin{tabular}{|r|l|r|}
%\hline
%Parameter & Value &\\
%\hline
%$r$ & \Sexpr{signif(c(flsp@params['r',]),3)} &\\
%$k$ & \Sexpr{signif(c(flsp@params['k',]),3)} &\\
%\hline
%Hessian & \Sexpr{signif(c(flsp@hessian[1,1,]),3)} & \Sexpr{signif(c(flsp@hessian[1,2,]),3)} \\
%				& \Sexpr{signif(c(flsp@hessian[2,1,]),3)} & \Sexpr{signif(c(flsp@hessian[2,2,]),3)} \\
%\hline
%Vcov & \Sexpr{signif(c(flsp@vcov[1,1,]),3)} & \Sexpr{signif(c(flsp@vcov[1,2,]),3)} \\
%     & \Sexpr{signif(c(flsp@vcov[2,1,]),3)} & \Sexpr{signif(c(flsp@vcov[2,2,]),3)} \\
%\hline
%\end{tabular}
%\caption{Results of fitting the model}.
%\label{tab:est_values}
%\end{table}
%
\section{Missing data and the impact on confidence}

The first question to answer is: what happens if we only have index data every
other year?
We would expect the confidence in our assessment to go down.
Does it? Let's find out.

<<label=setting_up_index_holes>>=
idxyrs <- as.numeric(dimnames(index)$year)
nayears1 <- seq(from=idxyrs[1],to=idxyrs[length(idxyrs)],by=2)
index_holes1 <- index
index_holes1[,nayears1] <- NA
@

Now set up another $FLsp$ object and fit.

<<label=best_assessment, quiet=TRUE>>=
flsp_holes1 <- FLsp(catch=catch, index=index_holes1)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp_holes1 <- fitsp(flsp_holes1, lower=lower, upper=upper,
 control=DEoptim.control(itermax=1000, trace=200))
@

% Table of parameter comparison, vcov, hessian, and likelihood
% Now moved into one big table of results
%\begin{table}
%\begin{tabular}{|r|l|r|}
%\hline
%Parameter & Value &\\
%\hline
%$r$ & \Sexpr{signif(c(flsp_holes1@params['r',]),3)} &\\
%$k$ & \Sexpr{signif(c(flsp_holes1@params['k',]),3)} &\\
%\hline
%Hessian & \Sexpr{signif(c(flsp_holes1@hessian[1,1,]),3)} & \Sexpr{signif(c(flsp_holes1@hessian[1,2,]),3)} \\
%				& \Sexpr{signif(c(flsp_holes1@hessian[2,1,]),3)} & \Sexpr{signif(c(flsp_holes1@hessian[2,2,]),3)} \\
%\hline
%Vcov & \Sexpr{signif(c(flsp_holes1@vcov[1,1,]),3)} & \Sexpr{signif(c(flsp_holes1@vcov[1,2,]),3)} \\
%     & \Sexpr{signif(c(flsp_holes1@vcov[2,1,]),3)} & \Sexpr{signif(c(flsp_holes1@vcov[2,2,]),3)} \\
%\hline
%\end{tabular}
%\caption{Results of fitting the model with holes in the index}.
%\label{tab:est_values_holes}
%\end{table}
%
Althought the estimated parameter values have not changed by much, the values in the
variance-covariance matrix of the estimated parameters has increased
which means that we are less confident in the results of the assessment (Table~\ref{tab:all_results}).

Let's do it again, this time with even more holes. This time there is one year of data to
two years of missing data:

<<label=setting_up_index_holes2>>=
nayears2 <- seq(from=idxyrs[1],to=idxyrs[length(idxyrs)],by=3)
index_holes2 <- index
index_holes2[,!(idxyrs %in% nayears2)] <- NA
flsp_holes2 <- FLsp(catch=catch, index=index_holes2)
lower <- c(1e-4,max(catch))
upper <- c(4,1e6)
flsp_holes2 <- fitsp(flsp_holes2, lower=lower, upper=upper,
 control=DEoptim.control(itermax=1000, trace=200))
@

% Now moved into one big table of results
%\begin{table}
%\begin{tabular}{|r|l|r|}
%\hline
%Parameter & Value &\\
%\hline
%$r$ & \Sexpr{signif(c(flsp_holes2@params['r',]),3)} &\\
%$k$ & \Sexpr{signif(c(flsp_holes2@params['k',]),3)} &\\
%\hline
%Hessian & \Sexpr{signif(c(flsp_holes2@hessian[1,1,]),3)} & \Sexpr{signif(c(flsp_holes2@hessian[1,2,]),3)} \\
%				& \Sexpr{signif(c(flsp_holes2@hessian[2,1,]),3)} & \Sexpr{signif(c(flsp_holes2@hessian[2,2,]),3)} \\
%\hline
%Vcov & \Sexpr{signif(c(flsp_holes2@vcov[1,1,]),3)} & \Sexpr{signif(c(flsp_holes2@vcov[1,2,]),3)} \\
%     & \Sexpr{signif(c(flsp_holes2@vcov[2,1,]),3)} & \Sexpr{signif(c(flsp_holes2@vcov[2,2,]),3)} \\
%\hline
%\end{tabular}
%\caption{Results of fitting the model with even more holes in the index}.
%\label{tab:est_values_holes2}
%\end{table}
%
The results of all three models can be see in Table~\ref{tab:all_results}.

The estimated parameter values and the corresponding values of $B_{current}$, $MSY$ and $B_{MSY}$ are all very similar.
However, the values in the variance-covariance matrix have increased (Table~\ref{tab:all_results}), again
suggesting tha confidence in these results has decreased.

What are the consequences of this decreased confidence?

% Ubertable of results
<<label=df_of_three_spresults,echo=FALSE>>=
temp <- data.frame(Measure=c("r","k","Hess_rr","Hess_rk","Hess_kr","Hess_kk","Vcov_rr","Vcov_rk","Vcov_kr","Vcov_kk","Bcurrent","MSY","Bmsy"),
									All_index_yrs=c(c(flsp@params),c(flsp@hessian),c(flsp@vcov),speedy_bcurrent(flsp),Msy(flsp),Bmsy(flsp)),
									Holes_index_1=c(c(flsp_holes1@params),c(flsp_holes1@hessian),c(flsp_holes1@vcov),speedy_bcurrent(flsp_holes1),Msy(flsp_holes1),Bmsy(flsp_holes1)),
									Holes_index_2=c(c(flsp_holes2@params),c(flsp_holes2@hessian),c(flsp_holes2@vcov),speedy_bcurrent(flsp_holes2),Msy(flsp_holes2),Bmsy(flsp_holes2))
									)
@

<<label=write_table, results=tex, echo=FALSE>>=
print(xtable(temp,
				caption="Assessment results for three data sets with increasing holes in index data",
				label="tab:all_results",
				digits=-2
				))
#				floating.environment = 'sidewaystable')
@

\section{Consequences of Confidence}

We can use the variance-covariance matrices to produce bootstrapped estimates of $r$ and $k$,
and therefore bootstrapped estimates of $B_{current}$, $MSY$ and $B_{MSY}$.

This can be done using a Cholesky decomposition of the variance-covariance matrix.
This method makes a bunch of assumptions blah blah.

The method can be put into a function:

<<label=conf_func>>=
conf_sp <- function(sp,nsam=1000)
{
	# Do we have vcov
	sp@vcov
	A <- t(chol(vcov(sp)[,,1]))

	Y <- matrix(nrow=nsam,ncol=dim(params(sp))[1])
	Y[] <- rnorm(nsam*ncol(Y))
	X <- t(apply(Y,1,function(x,A){x <- A%*%x},A))
	# Correct for mean
	X <- sweep(X,2,params(sp),"+")

	# Remove negatives
	#X <- X[apply(X,1,function(x) any(x > 0)),]

	# Make a new object
	sp_sim <- sp
	params(sp_sim) <- propagate(params(sp_sim), nrow(X))
	params(sp_sim)[] <- t(X)

	return(sp_sim)
}
@

This function can then be used to generate bootstrapped $FLsp$ objects.

<<label=bootstrap_assessments>>=
flsp_boot <- conf_sp(flsp)
flsp_holes1_boot <- conf_sp(flsp_holes1)
flsp_holes2_boot <- conf_sp(flsp_holes2)
@

\begin{figure}
<<label=histo_r_k,echo=FALSE,fig=T>>=
rkdf <- rbind(cbind(index="full",as.data.frame(sweep(flsp_boot@params,1,flsp@params,"/"))),
							cbind(index="holes1",as.data.frame(sweep(flsp_holes1_boot@params,1,flsp_holes1@params,"/"))),
							cbind(index="holes2",as.data.frame(sweep(flsp_holes2_boot@params,1,flsp_holes2@params,"/"))))

print(ggplot(rkdf,aes(x=data)) + geom_histogram() + facet_grid(index ~ params))
@
\caption{Histograms of the bootstrapped $r$ and $k$ values, relative to the 'true' value}
\label{fig:rk_hist}
\end{figure}

Histograms of the bootstrapped values of $r$ and $k$, relative to the 'true' value can be seen in Figure~\ref{fig:rk_hist}.

\begin{figure}
<<label=boot_MSYetc,echo=FALSE, fig=TRUE>>=
msyetcdf <- rbind(data.frame(index="full", MSY=Msy(flsp_boot),BMSY = Bmsy(flsp_boot), BC = speedy_bcurrent(flsp_boot)),
						data.frame(index="holes1", MSY=Msy(flsp_holes1_boot),BMSY = Bmsy(flsp_holes1_boot), BC = speedy_bcurrent(flsp_holes1_boot)),
						data.frame(index="holes2", MSY=Msy(flsp_holes2_boot),BMSY = Bmsy(flsp_holes2_boot), BC = speedy_bcurrent(flsp_holes2_boot)))
# melt this to single value column
meltdf <- melt(msyetcdf,id.vars="index")
print(ggplot(meltdf,aes(x=value)) + geom_histogram() + facet_grid(index ~ variable, scales="free"))
@
\caption{Histograms of derived measures for the three index holes scenarios}
\label{fig:hist_msy_holes}
\end{figure}

We can do the same thing for the derived measures $B_{current}$, $MSY$ and $B_{MSY}$ (Figure~\ref{fig:hist_msy_holes}).

This clearly shows that as the number of holes in the index data increases, the confidence in the estimated parameters $r$ and $k$ and
the derived measures decreases.

Some of the bootstrapped pairs of values of $r$ and $k$ actually result in a value
of 0 for $B_{current}$, i.e. the combination of $r$ and $k$ has led to the population
crashing.

What does this mean?
The histograms show the uncertainty
in the best estimate of $B_{current}$ but at the same time we know that the population
hasn't actually crashed.

This method is only approximate. The generated pairs of values of $r$ and $k$ might be
statistically consistent with the covariance matrix, but the covariance matrix is
only an approximation to the 'true' confidence in the parameters. Clearly we cannot have
pairs of parameter values that are not likely (i.e. they lead to the population crashing).

We therefore have to do some tidying up of the generated pairs of values.

<<label=getLL_of_rk_pairs_holes2, echo=FALSE>>=
ll_holes2 <- ll(flsp_holes2_boot)
@

First let's take a look at the likelihood of each pair of parameter values. Out of the
\Sexpr{length(c(ll_holes2))} values, \Sexpr{sum(is.nan(c(ll_holes2)))} are NaN.
There are \Sexpr{sum(is.nan(c(ll_holes2)))} pairs of $r$ and $k$ values that
lead to invalid likelihoods. This happens when the population crashes.

% Laurie said to plot the sorted likelihoods.
% He said it would be sigmoid shape - think this is only the case when you have a continuous LL
% i.e. set minimim B to 1e-9 or something - so you can get LLs even when r and k are weird
% So he weights the likelihood. i..e. if pop crash, massively negative LL
% We don't do that because we are using a min pop of 0, so we get NaN if pop crashes
% Just checked and this is true...

% So the next step only matters if we have Bmin != 0?
% and why do it at all?
% Scale between 0 and 1.
% Pick uniform random number between 0 and 1. Sample from LLs, with replacement.
% How will this be different

<<label=look_at_LLs_holes2,echo=FALSE>>=
plot(sort(c(ll_holes2)))
@


Quick tests have shown that almost all bootstrapped Bcurrent values are 0 for
with the 'one-way-trip' scenario when maxFfactor = 2.



\section{References}

Sweave
Polacheck


\end{document}

